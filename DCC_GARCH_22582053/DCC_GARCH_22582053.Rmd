---
# IMPORTANT: Change settings here, but DO NOT change the spacing.
# Remove comments and add values where applicable.
# The descriptions below should be self-explanatory

title: "MV-Volatility modeling applied to local equities"
#subtitle: "This will appear as Right Header"

documentclass: "elsarticle"

# --------- Thesis title (Optional - set to FALSE by default).
# You can move the details below around as you please.
Thesis_FP: FALSE
# Entry1: "An unbelievable study with a title spanning multiple lines."
# Entry2: "\\textbf{Some Guy}" # textbf for bold
# Entry3: "A thesis submitted toward the degree of Doctor of Philosophy"
# Uni_Logo: Tex/Logo.png # Place a logo in the indicated location (from your root, e.g. defaults to ~/Tex/Logo.png) and uncomment this line. Leave uncommented for no image
# Logo_width: 0.3 # If using a logo - use this to set width (size) of image
# Entry4: "Under the supervision of: \\vfill Prof. Joe Smith and Dr. Frank Smith"
# Entry5: "Stellenbosch University"
# Entry6: April 2020
# Entry7:
# Entry8:

# --------- Front Page
# Comment: ----- Follow this pattern for up to 5 authors
AddTitle: TRUE # Use FALSE when submitting to peer reviewed platform. This will remove author names.
Author1: "Austin Byrne"  # First Author - note the thanks message displayed as an italic footnote of first page.
Ref1: "Stellenbosch University" # First Author's Affiliation
Email1: "22582053\\@sun.ac.za" # First Author's Email address

#Author2: "John Smith"
#Ref2: "Some other Institution, Cape Town, South Africa"
#Email2: "John\\@gmail.com"
#CommonAffiliation_12: TRUE # If Author 1 and 2 have a common affiliation. Works with _13, _23, etc.

#Author3: "John Doe"
#Email3: "Joe\\@gmail.com"

CorrespAuthor_1: TRUE  # If corresponding author is author 3, e.g., use CorrespAuthor_3: TRUE

# Comment out below to remove both. JEL Codes only given if keywords also given.
#keywords: "Multivariate GARCH \\sep Kalman Filter \\sep Copula" # Use \\sep to separate
#JELCodes: "L250 \\sep L100"

# ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage" # Add a '#' before this line to remove footer.
addtoprule: TRUE
addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# --------- page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top
HardSet_layout: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashing text to fit on pages, e.g.
# This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper.

# --------- Line numbers
linenumbers: FALSE # Used when submitting to journal

# ---------- References settings:
# You can download cls format here: https://www.zotero.org/ - simply search for your institution. You can also edit and save cls formats here: https://editor.citationstyles.org/about/
# Hit download, store it in Tex/ folder, and change reference below - easy.
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
csl: Tex/harvard-stellenbosch-university.csl # referencing format used.
# By default, the bibliography only displays the cited references. If you want to change this, you can comment out one of the following:
#nocite: '@*' # Add all items in bibliography, whether cited or not
# nocite: |  # add specific references that aren't cited
#  @grinold2000
#  @Someoneelse2010

# ---------- General:
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: FALSE                       # Add a table of contents
numbersections: TRUE             # Should sections (and thus figures and tables) be numbered?
fontsize: 12pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.

### Adding additional latex packages:
# header-includes:
#    - \usepackage{colortbl} # Add additional packages here.

output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
abstract: |
  The main aim of this study is to evaluate the volatility structure of local equities. This analysis will be achieved by making use of DCC-GARCH multivariate volatility model. Through this analysis I indetify the most volatile index/sector to be the Resource sector, while the ALSI and Industrial sector to hold the lowest volatility. 
---

<!-- First: Set your default preferences for chunk options: -->

<!-- If you want a chunk's code to be printed, set echo = TRUE. message = FALSE stops R printing ugly package loading details in your final paper too. I also suggest setting warning = FALSE and checking for warnings in R, else you might find ugly warnings in your paper. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf.
# These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!

# Lets load in example data, and see how this can be stored and later called from your 'data' folder.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
library(MTS)
library(robustbase)
library(gt)
library(dplyr)
library(lubridate)
library(zoo)
library(tidyr)
library(tseries)
library(PortfolioAnalytics)
library(ggplot2)
library(RcppRoll)
library(tbl2xts)
library(fmxdat)
library(rmsfuns)
library(PerformanceAnalytics)
list.files('code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))


```

```{r}
#Loading the relevant data
LCL_Indices <- readRDS("C:/Users/austi/OneDrive/Desktop/Masters/Financial Econometrics/Project/DCC_GARCH_volatility_model/data/LCL_Indices (1).rds")
#LCL_Stock_Returns <- readRDS("C:/Users/austi/OneDrive/Desktop/Masters/Financial Econometrics/Project/DCC_GARCH_volatility_model/data/LCL_Stock_Returns (1).rds")
```

```{r, include=FALSE}
#Checking how many total observations we have
nrow(LCL_Indices)

# For LCL_Indices data set
sum(is.na(LCL_Indices))

# For LCL_Stock_Returns data set
#sum(is.na(LCL_Stock_Returns))
```

```{r, include=FALSE}
#Creating a new cleaned data set that has no NA's 
LCL_Indices_Cleaned <- na.omit(LCL_Indices)

#Evaluating whether the NA's have been removed successfully
sum(is.na(LCL_Indices_Cleaned))

#Evaluating the amount of data still available
nrow(LCL_Indices_Cleaned)
```


```{r}
# Reshaping the LCL_Indices_Cleaned data to a wide format
LCL_Indices_wide <- LCL_Indices_Cleaned %>%
  select(date, Name, Returns) %>%
  spread(Name, Returns)

```

```{r}
# Calculating the correlation matrix for index returns
cor_matrix <- cor(LCL_Indices_wide[,-1], use = "complete.obs")

```

```{r}
# Setting a threshold for identifying high correlations
high_corr_threshold <- 0.9

# Finding index pairs with correlations above the threshold
high_corr_indices <- which(abs(cor_matrix) > high_corr_threshold, arr.ind = TRUE)

# Excluding self-correlation (diagonal elements)
high_corr_indices <- high_corr_indices[high_corr_indices[, 1] != high_corr_indices[, 2], ]

# Ensuring unique pairs are identified to avoid duplication
unique_high_corr_indices <- unique(t(apply(high_corr_indices, 1, sort)))

# Associating the indices names with the high correlation pairs
high_corr_pairs <- apply(unique_high_corr_indices, 1, function(idx) {
  return(colnames(cor_matrix)[idx])
})

```

```{r}
# Define the indices to include in the analysis
selected_indices <- c("ALSI", "Financials", "Industrials", "jsapy", "Resources")

# Filter the dataset to keep only the selected indices
LCL_Indices_Selected <- LCL_Indices_Cleaned %>%
  filter(Name %in% selected_indices)
```


<!-- ############################## -->
<!-- # Start Writing here: -->
<!-- ############################## -->

# Introduction \label{Introduction}

When dealing with financial markets and portfolio creation it is paramount for an investor or risk managers to understand the risk involved in their investment decisions. Within financial markets, volatility is often evaluated as a measure of uncertainty and risk. Thus, through evaluating the volatility structure of indexes we are able to understand the risk structure involved with certain investment decisions. Furthermore, by evaluating the dynamic correlations between indexes we are able to better diversify a portfolio and reduce risk. The main objective of this project is to understand the multivariate volatility structure of local equities and thus understand the relative risk associated with certain investment decisions. 

The use of Dynamic Conditional Correlation (DCC) GARCH models to evaluate market dynamics and dynamic correlations has become more frequent in portfolio creation and financial theory. @boudt2013robust make use of an extension of a DCC GARCH model to forecasting the covariance matrix of the daily EUR/USD and Yen/USD return series [@boudt2013robust]. Additionally, @shiferaw2019time run a multivariate DCC-GARCH model to evaluate time varying correlation between energy price dynamics and agricultural commodity to name a few [@shiferaw2019time]. 

Understanding this topic can be vitally important for investors and portfolio managers when making investment decisions. Realizing the risk exposure a portfolio is inherently exposed to is crucial when making investment decisions. Thus, by modeling the volatility of local equities an investor can make more informed investment decisions based on the intended level of risk the investors wants to take on. This project aims to achieve this analysis through a Dynamic Conditional Correlation (DCC) GARCH model capturing the market dynamics and volatility structures of local sectors and indexes. This model is renowned for its proficiency in assessing multivariate time series data, particularly its capability to capture the evolving correlations between different financial time series. 

The results of this study find the Resource sector to be the most volatile with the ALSI and Industrial sector being the least volatile. 

The remainder of this paper is ordered as follows: The next section covers the Data and Methodology, followed by the Results section, followed by the discussion section and lastly the Conclusion. 



# Data Methodology \label{Data Methodology}

Throughout this paper multivariate volatility modeling techniques will be used with the focus on understanding the dynamic relationships and co-movements of volatility across multiple sectors/indexes within the local equities market. 

Of the MV-volatility modeling techniques that there are available, I will be utilizing the DCC-GARCH model. My aim is to use DCC models in the multivariate volatility analysis as these models are simpler and relax the constraint of a fixed correlation structure which is assumed by the CCC model, which allows for estimates of time varying correlation. I will be conducting the DCC-GARCH volatility model in R statistical package. 

The data I am using holds the returns for the following indexes/sectors, ALSI, Financial Sector, Industrial Sector, jsapy index and the Resource Sector.

## Variation in index returns 

Figure \ref{Figure1} below plots the time series analysis of each indexes returns. The importance of this plot for this analysis lies in it's ability to portray the variance structure in each indexes returns. By visualizing the return structure of an index through time we are able to identify the most volatile and thus more risky indexes. 

While holding the y-axis fixed, figure \ref{Figure1} shows that the most volatile index/sector is that of the Resource sector.  While the ALSI and Industrial sector appear to be the least volatile and thus lowest risk investment opportunities. 

To further evaluate the return structure of the indexes/sectors in question figure \ref{Figure2} analyses the distribution of returns through a histogram plot. 

```{r Figure1,  warning =  FALSE, fig.align = 'center', fig.cap = "Time Series Analysis of Returns \\label{Figure1}", fig.ext = 'png', fig.height = 5, fig.width = 6}

# Using ggplot2 to create time series plots of the selected indices' returns
Index_return_plot <- ggplot(LCL_Indices_Selected, aes(x = date, y = Returns, color = Name)) + 
  geom_line() + 
  facet_wrap(~Name, scales = "fixed") +  # Ensures comparable y-axis scales across facets
  theme_minimal() +
  ggtitle("Time Series Plot of Indices Returns with Fixed Y-Axis")

# Displaying the plot
Index_return_plot


```

## Analyzing the Distribution of Returns

Through figure \ref{Figure1} we have identified the resource sector as the most volatile and thus most risky investment option, while the ALSI and industrial sector to be the least volatile. To build on this analysis I run a histogram plot of the return structure. Through this plot we are able to identify any fat tails or skewness in returns as well as identify the index/sector with the widest variation in returns. 

The results of figure \ref{Figure2} below are inline with that found in figure \ref{Figure1} above. The resource sector once again appears to be the most volatile with the ALSI and industrial sector appearing to be the least volatile. 

Following the analysis of figures \ref{Figure1} and \ref{Figure2} we have now obtained a better understanding of the return structure and volatility of the indexes/sectors in question. Thus, we are now able to proceed to the multivariate volatility analysis using the DCC-GARCH model. However, Before I fit any GARCH models to my data I need evaluate the presence of any ARCH effects. If there are no ARCH effects present, it does not make sense to run any GARCH models for this volatility analysis. To test for ARCH effects I perform the McLeod-Li test. This test checks for auto regressive conditional heteroskedasticity (ARCH) in the time series data [@wang2005testing]. If present the p-values will be very close to 0, indicating that the volatility within the variables changes over time. Thus, it is viable to use GARCH models. This test is done in the next section.  

```{r Figure2,  warning =  FALSE, fig.align = 'center', fig.cap = "Distribution of Returns \\label{Figure2}", fig.ext = 'png', fig.height = 5, fig.width = 6}

# Creating histograms for the distribution of returns for each index
Return_distribution_hist <- ggplot(LCL_Indices_Selected, aes(x = Returns, fill = Name)) + 
  geom_histogram(bins = 30, color = "black") +
  facet_wrap(~Name, scales = "fixed") +  # Fixed scales for x-axis across facets
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Tilting x-axis labels for readability
  ggtitle("Distribution of Returns for Indices with Fixed X-Axis")

Return_distribution_hist

```


## Testing for ARCH Effects

The results in table 2.1 below provide evidence of ARCH effects. All p-values are close to zero which indicates that there is statistical significance indicating that the volatility between the variables change over time. Thus, it is viable to make use of a GARCH model to conduct the volatility analysis. 

In the next section I will make use of the DCC-GARCH model to plot the Dynamic Conditional Correlation plots and report the results. 


```{r}
# Calculating the date range for each index
date_ranges <- LCL_Indices_Selected %>%
  group_by(Name) %>%
  summarize(Start_Date = min(date, na.rm = TRUE),
            End_Date = max(date, na.rm = TRUE))
```

```{r}
# Defining a consistent time range for the analysis
start_date <- as.Date("2002-02-28")  # Start date
end_date <- as.Date("2023-11-30")    # End date

# Filtering the data set for the defined time range
LCL_Indices_Filtered <- LCL_Indices_Selected %>%
  filter(date >= start_date & date <= end_date)

```

```{r}
# Loading necessary libraries
pacman::p_load("tbl2xts", "MTS")

# Converting the filtered dataset to an xts object for time series analysis
xts_rtn <- LCL_Indices_Filtered %>% tbl_xts(., cols_to_xts = "Returns", spread_by = "Name")
```


```{r ,results='hide'}
# Performing the McLeod-Li test to check for ARCH effects
MarchTest(xts_rtn)
```

```{r}

test_results <- data.frame(
  Test = c("Q(m) of squared series (LM test)", 
           "Rank-based Test", 
           "Q_k(m) of squared series", 
           "Robust Test (5%)"),
  TestStatistic = c(44.88386, 108.2555, 341.5009, 321.4297),
  PValue = c(2.282185e-06, 0, 0.000104816, 0.001529345)
)

# Using knitr to create a LaTeX formatted table
knitr::kable(test_results, format = "latex", booktabs = TRUE,
             caption = "McLeod-Li Test Results for Autoregressive Conditional Heteroskedasticity")

```

<!-- Make title of bibliography here: -->
<!-- \newpage -->

\newpage

# References {-}

<div id="refs"></div>


# Appendix {-}

## Appendix A {-}

Some appendix information here

## Appendix B {-}

